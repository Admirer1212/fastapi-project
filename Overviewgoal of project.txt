https://forms.office.com/r/BgbfXRyJTZ

Overview/goal of project

Build an AI-powered system that analyzes customer-uploaded images on the gluu.repair website.
Predict the type of shoe damage or related issue using an AI model.
Recommend a service (e.g., "Leather Care" or "Cleaning Service") based on the prediction.
Integrate this functionality seamlessly into your Bubble.io front-end for customer interaction.



laion2B data set is used for training

worked on removing unwanted line items -100  lines 
changed the hardcoded file names in the code it will dynamically pick the file names from the mentioned folder
defined the use of fast api and ngork 
pytorch doensot support the latest version of python so the python version has been degraded
api initialization in the bubble.io platform 
issue in parsing the response from fast api in bubble.io

- fastapi
- ngork public hosting (Ngrok tunnels the request to expose FastAPI externally)
  The error "The host 127.0.0.1 is not valid" occurs because 127.0.0.1 (localhost) is only accessible from your        local machine. Bubble.io is a cloud-based service and cannot access your locally running FastAPI server.

âœ… Solution: Expose FastAPI to the Internet
To make FastAPI accessible to Bubble.io, you need to host it on a public server. There are multiple ways to do this:

Option 1: Use Ngrok (Best for Local Testing)
Ngrok creates a temporary public URL that routes to your local FastAPI server.
clip processor/laion2b data set 

########### old main code



#### Main.py script Accepts Images from the Front-End(bubble.io) via an Endpoint ######

from fastapi import FastAPI, UploadFile, File
from fastapi.responses import JSONResponse
from transformers import CLIPProcessor, CLIPModel
from PIL import Image
import os
import torch

# Initialize FastAPI app
app = FastAPI()

# Create uploads folder
UPLOAD_FOLDER = "uploads"
os.makedirs(UPLOAD_FOLDER, exist_ok=True)

# Define global labels
LABELS = [
    "Broken stitches", "Opened seam", "Slanted heel", "Sole not flat",
    "Peeling leather/tears", "Weak cementing", "Wrinkles",
    "Dirty shoes", "Leather faded"
]

# Define a dynamic mapping for "Should Be" labels
SHOULD_BE_MAPPING = {
    "Broken stitches": "Leather Care",
    "Opened seam": "Stitch Repair",
    "Slanted heel": "Heel Adjustment",
    "Sole not flat": "Sole Flattening",
    "Peeling leather/tears": "Leather Customization RE-DYE",
    "Weak cementing": "Re-Cementing",
    "Wrinkles": "Leather Customization RE-DYE",
    "Dirty shoes": "Cleaning Service",
    "Leather faded": "Leather Restoration"
}

# Load the CLIP model and processor
model = CLIPModel.from_pretrained("laion/CLIP-ViT-H-14-laion2B-s32B-b79K")
processor = CLIPProcessor.from_pretrained("laion/CLIP-ViT-H-14-laion2B-s32B-b79K")

def process_image(image_path):
    """
    Processes an image, predicts the label, and returns the formatted result.
    """
    try:
        # Extract the filename
        image_name = os.path.basename(image_path)
        print(f"Processing image: {image_name}")  # Debug log

        # Open and preprocess the image
        image = Image.open(image_path).convert("RGB")
        inputs = processor(
            text=LABELS,
            images=image,
            return_tensors="pt",
            padding=True
        )

        # Run the model
        outputs = model(
            pixel_values=inputs["pixel_values"],
            input_ids=inputs["input_ids"],
            attention_mask=inputs["attention_mask"]
        )
        probs = outputs.logits_per_image.softmax(dim=1)  # Convert logits to probabilities
        predicted_idx = torch.argmax(probs, dim=1).item()
        predicted_label = LABELS[predicted_idx]

        # Dynamically determine the "Should Be" label
        should_be_label = SHOULD_BE_MAPPING.get(predicted_label, "Unknown Recommendation")

        # Format the output
        result = (
            f"Image {image_name} is predicted to be: {predicted_label}\n"
            f"Should be: {should_be_label}"
        )
        print(result)  # Debug log
        return result
    except Exception as e:
        print(f"Error: {str(e)}")
        raise RuntimeError(f"Error processing image: {str(e)}")

@app.post("/process-image/")
async def process_image_endpoint(file: UploadFile = File(...)):
    """
    Endpoint to upload and process an image.
    """
    try:
        # Save the uploaded file
        file_path = os.path.join(UPLOAD_FOLDER, file.filename)
        with open(file_path, "wb") as buffer:
            buffer.write(await file.read())

        # Process the image and get the result
        result = process_image(file_path)

        # Clean up
        os.remove(file_path)

        # Return the result as JSON
        return JSONResponse(content={"success": True, "result": result})
    except Exception as e:
        return JSONResponse(content={"success": False, "error": str(e)}, status_code=500)


################ updated backup of main code 6-feb-2025

#### Updated Main.py - AI-Powered Shoe Damage & Material Detection API ######

from fastapi import FastAPI, UploadFile, File
from fastapi.responses import JSONResponse
from transformers import CLIPProcessor, CLIPModel
from PIL import Image
import os
import torch
import hashlib

# Initialize FastAPI app
app = FastAPI()

# Create uploads folder
UPLOAD_FOLDER = "uploads"
os.makedirs(UPLOAD_FOLDER, exist_ok=True)

# Define labels related to shoe damage identification
LABELS = [
    "Broken stitches", "Opened seam", "Slanted heel", "Sole not flat",
    "Peeling leather/tears", "Weak cementing", "Wrinkles",
    "Dirty shoes", "Leather faded"
]

# Define possible shoe materials for AI classification
MATERIAL_LABELS = ["Leather", "Canvas", "Suede", "Synthetic", "Rubber"]

# Define appropriate repair recommendations
REPAIR_MAPPING = {
    "Broken stitches": "Leather Care",
    "Opened seam": "Stitch Repair",
    "Slanted heel": "Heel Adjustment",
    "Sole not flat": "Sole Flattening",
    "Peeling leather/tears": "Leather Customization RE-DYE",
    "Weak cementing": "Re-Cementing",
    "Wrinkles": "Leather Customization RE-DYE",
    "Dirty shoes": "Cleaning Service",
    "Leather faded": "Leather Restoration"
}

# Define fixed metadata categories
ERA_MAPPING = ["Vintage", "Modern", "Classic", "Retro", "Antique"]
VALUE_MAPPING = ["Low", "Medium", "High", "Rare", "Collector's Item"]
CONDITION_MAPPING = {
    "Broken stitches": "Fair",
    "Opened seam": "Fair",
    "Slanted heel": "Good",
    "Sole not flat": "Poor",
    "Peeling leather/tears": "Poor",
    "Weak cementing": "Very Poor",
    "Wrinkles": "Fair",
    "Dirty shoes": "Good",
    "Leather faded": "Fair"
}

# Load the pre-trained CLIP model for image classification
model = CLIPModel.from_pretrained("laion/CLIP-ViT-H-14-laion2B-s32B-b79K")
processor = CLIPProcessor.from_pretrained("laion/CLIP-ViT-H-14-laion2B-s32B-b79K")

def compute_image_hash(image_path):
    """ Generates a stable hash based on the image content."""
    hasher = hashlib.sha256()
    with open(image_path, 'rb') as img_file:
        while chunk := img_file.read(8192):
            hasher.update(chunk)
    return hasher.hexdigest()

def stable_hash(hash_str, choices):
    """ Uses a consistent hash from image content to determine metadata values."""
    hash_val = int(hash_str, 16)
    return choices[hash_val % len(choices)]

def predict_material(image_path):
    """ Uses AI to predict the shoe material accurately."""
    image = Image.open(image_path).convert("RGB")
    inputs = processor(
        text=MATERIAL_LABELS,
        images=image,
        return_tensors="pt",
        padding=True
    )
    with torch.no_grad():
        outputs = model(
            pixel_values=inputs["pixel_values"],
            input_ids=inputs.get("input_ids"),
            attention_mask=inputs.get("attention_mask")
        )
    probs = outputs.logits_per_image.softmax(dim=1)  # Convert logits to probabilities
    predicted_idx = torch.argmax(probs, dim=1).item()
    return MATERIAL_LABELS[predicted_idx]

def process_image(image_path):
    """
    Uses AI to analyze the shoe image, predict damages and material, and return accurate repair recommendations.
    """
    try:
        # Compute a stable hash based on the image content
        image_hash = compute_image_hash(image_path)
        print(f"Processing image with hash: {image_hash[:10]}")  # Debug log


        # Open and preprocess the image
        image = Image.open(image_path).convert("RGB")
        inputs = processor(
            text=LABELS,
            images=image,
            return_tensors="pt",
            padding=True
        )

        # Run AI model for damage prediction
        with torch.no_grad():
            outputs = model(
                pixel_values=inputs["pixel_values"],
                input_ids=inputs.get("input_ids"),
                attention_mask=inputs.get("attention_mask")
            )
            
        probs = outputs.logits_per_image.softmax(dim=1)  # Convert logits to probabilities
        predicted_idx = torch.argmax(probs, dim=1).item()
        predicted_label = LABELS[predicted_idx]
        repair_suggestion = REPAIR_MAPPING.get(predicted_label, "Unknown Repair Suggestion")

        # AI-based shoe material classification
        predicted_material = predict_material(image_path)

        # Stable metadata generation based on image content hash
        additional_details = {
            "name_of_product": f"Product-{int(image_hash[:8], 16) % 10000}",
            "era": stable_hash(image_hash, ERA_MAPPING),
            "material": predicted_material,  # AI-determined material
            "condition": CONDITION_MAPPING.get(predicted_label, "Unknown"),
            "value": stable_hash(image_hash, VALUE_MAPPING)
        }

        # Format response
        result = {
            "image_name": os.path.basename(image_path),
            "predicted_damage": predicted_label,
            "recommended_repair": repair_suggestion,
            "name_of_product": additional_details["name_of_product"],
            "era": additional_details["era"],
            "material": additional_details["material"],
            "condition": additional_details["condition"],
            "value": additional_details["value"]
        }

        print(result)  # Debug log
        return result
    except Exception as e:
        print(f"Error: {str(e)}")
        raise RuntimeError(f"Error processing image: {str(e)}")

@app.post("/process-image/")
async def process_image_endpoint(file: UploadFile = File(...)):
    """
    Endpoint to upload and process an image, returning AI-generated shoe damage and material analysis.
    """
    try:
        # Save uploaded file temporarily
        file_path = os.path.join(UPLOAD_FOLDER, file.filename)
        with open(file_path, "wb") as buffer:
            buffer.write(await file.read())

        # Process image using AI model
        result = process_image(file_path)

        # Clean up
        os.remove(file_path)

        # Return AI-generated analysis
        return JSONResponse(content={"success": True, "result": result})
    except Exception as e:
        return JSONResponse(content={"success": False, "error": str(e)}, status_code=500)

############## updated backup 06-feb v1

from fastapi import FastAPI, UploadFile, File
from fastapi.responses import JSONResponse
from transformers import CLIPProcessor, CLIPModel
from PIL import Image
import os
import torch
import hashlib

# Initialize FastAPI app
app = FastAPI()

# Create uploads folder
UPLOAD_FOLDER = "uploads"
os.makedirs(UPLOAD_FOLDER, exist_ok=True)

# Define labels related to shoe damage identification
LABELS = [
    "Broken stitches", "Opened seam", "Slanted heel", "Sole not flat",
    "Peeling leather/tears", "Weak cementing", "Wrinkles",
    "Dirty shoes", "Leather faded"
]

# Define possible shoe materials for AI classification
MATERIAL_LABELS = ["Leather", "Canvas", "Suede", "Synthetic", "Rubber"]

# Define appropriate repair recommendations
REPAIR_MAPPING = {
    "Broken stitches": "Leather Care",
    "Opened seam": "Stitch Repair",
    "Slanted heel": "Heel Adjustment",
    "Sole not flat": "Sole Flattening",
    "Peeling leather/tears": "Leather Customization RE-DYE",
    "Weak cementing": "Re-Cementing",
    "Wrinkles": "Leather Customization RE-DYE",
    "Dirty shoes": "Cleaning Service",
    "Leather faded": "Leather Restoration"
}

# Define fixed metadata categories
ERA_MAPPING = ["Vintage", "Modern", "Classic", "Retro", "Antique"]
VALUE_MAPPING = ["Low", "Medium", "High", "Rare", "Collector's Item"]
CONDITION_MAPPING = {
    "Broken stitches": "Fair",
    "Opened seam": "Fair",
    "Slanted heel": "Good",
    "Sole not flat": "Poor",
    "Peeling leather/tears": "Poor",
    "Weak cementing": "Very Poor",
    "Wrinkles": "Fair",
    "Dirty shoes": "Good",
    "Leather faded": "Fair"
}

# Load the pre-trained CLIP model for image classification
model = CLIPModel.from_pretrained("laion/CLIP-ViT-H-14-laion2B-s32B-b79K")
processor = CLIPProcessor.from_pretrained("laion/CLIP-ViT-H-14-laion2B-s32B-b79K")

def compute_image_hash(image_path):
    """ Generates a stable hash based on the image content."""
    hasher = hashlib.sha256()
    with open(image_path, 'rb') as img_file:
        while chunk := img_file.read(8192):
            hasher.update(chunk)
    return hasher.hexdigest()

def stable_hash(hash_str, choices):
    """ Uses a consistent hash from image content to determine metadata values."""
    hash_val = int(hash_str, 16)
    return choices[hash_val % len(choices)]

def predict_material(image_path):
    """ Uses AI to predict the shoe material accurately."""
    image = Image.open(image_path).convert("RGB")
    inputs = processor(
        text=MATERIAL_LABELS,
        images=image,
        return_tensors="pt",
        padding=True
    )
    with torch.no_grad():
        outputs = model(
            pixel_values=inputs["pixel_values"],
            input_ids=inputs.get("input_ids"),
            attention_mask=inputs.get("attention_mask")
        )
    probs = outputs.logits_per_image.softmax(dim=1)  # Convert logits to probabilities
    predicted_idx = torch.argmax(probs, dim=1).item()
    return MATERIAL_LABELS[predicted_idx]

def process_image(image_path):
    """
    Uses AI to analyze the shoe image, predict damages and material, and return accurate repair recommendations.
    """
    try:
        # Compute a stable hash based on the image content
        image_hash = compute_image_hash(image_path)
        print(f"Processing image with hash: {image_hash[:10]}")  # Debug log

        # Open and preprocess the image
        image = Image.open(image_path).convert("RGB")
        inputs = processor(
            text=LABELS,
            images=image,
            return_tensors="pt",
            padding=True
        )

        # Run AI model for damage prediction
        with torch.no_grad():
            outputs = model(
                pixel_values=inputs["pixel_values"],
                input_ids=inputs.get("input_ids"),
                attention_mask=inputs.get("attention_mask")
            )
            
        probs = outputs.logits_per_image.softmax(dim=1)  # Convert logits to probabilities
        predicted_idx = torch.argmax(probs, dim=1).item()
        predicted_label = LABELS[predicted_idx]
        repair_suggestion = REPAIR_MAPPING.get(predicted_label, "Unknown Repair Suggestion")

        # AI-based shoe material classification
        predicted_material = predict_material(image_path)

        # Stable metadata generation based on image content hash
        return {
            "success": True,
            "image_name": os.path.basename(image_path),
            "predicted_damage": predicted_label,
            "recommended_repair": repair_suggestion,
            "name_of_product": f"Product-{int(image_hash[:8], 16) % 10000}",
            "era": stable_hash(image_hash, ERA_MAPPING),
            "material": predicted_material,  # AI-determined material
            "condition": CONDITION_MAPPING.get(predicted_label, "Unknown"),
            "value": stable_hash(image_hash, VALUE_MAPPING)
        }

    except Exception as e:
        print(f"Error: {str(e)}")
        raise RuntimeError(f"Error processing image: {str(e)}")

@app.post("/process-image/")
async def process_image_endpoint(file: UploadFile = File(...)):
    """
    Endpoint to upload and process an image, returning AI-generated shoe damage and material analysis.
    """
    try:
        # Save uploaded file temporarily
        file_path = os.path.join(UPLOAD_FOLDER, file.filename)
        with open(file_path, "wb") as buffer:
            buffer.write(await file.read())

        # Process image using AI model
        result = process_image(file_path)

        # Clean up
        os.remove(file_path)

        # Return AI-generated analysis (Flattened Structure)
        return JSONResponse(content=result)
    except Exception as e:
        return JSONResponse(content={"success": False, "error": str(e)}, status_code=500)

######### Backup 08-feb-25

from fastapi import FastAPI, UploadFile, File
from fastapi.responses import JSONResponse
from pydantic import BaseModel
from transformers import CLIPProcessor, CLIPModel
from PIL import Image
import os
import torch
import hashlib

# Initialize FastAPI app
app = FastAPI()

# Create uploads folder
UPLOAD_FOLDER = "uploads"
os.makedirs(UPLOAD_FOLDER, exist_ok=True)

# Define labels related to shoe damage identification
LABELS = [
    "Broken stitches", "Opened seam", "Slanted heel", "Sole not flat",
    "Peeling leather/tears", "Weak cementing", "Wrinkles",
    "Dirty shoes", "Leather faded"
]

# Define possible shoe materials for AI classification
MATERIAL_LABELS = ["Leather", "Canvas", "Suede", "Synthetic", "Rubber"]

# Define appropriate repair recommendations
REPAIR_MAPPING = {
    "Broken stitches": "Leather Care",
    "Opened seam": "Stitch Repair",
    "Slanted heel": "Heel Adjustment",
    "Sole not flat": "Sole Flattening",
    "Peeling leather/tears": "Leather Customization RE-DYE",
    "Weak cementing": "Re-Cementing",
    "Wrinkles": "Leather Customization RE-DYE",
    "Dirty shoes": "Cleaning Service",
    "Leather faded": "Leather Restoration"
}

# Define fixed metadata categories
ERA_MAPPING = ["Vintage", "Modern", "Classic", "Retro", "Antique"]
VALUE_MAPPING = ["Low", "Medium", "High", "Rare", "Collector's Item"]
CONDITION_MAPPING = {
    "Broken stitches": "Fair",
    "Opened seam": "Fair",
    "Slanted heel": "Good",
    "Sole not flat": "Poor",
    "Peeling leather/tears": "Poor",
    "Weak cementing": "Very Poor",
    "Wrinkles": "Fair",
    "Dirty shoes": "Good",
    "Leather faded": "Fair"
}

# Load the pre-trained CLIP model for image classification
model = CLIPModel.from_pretrained("laion/CLIP-ViT-H-14-laion2B-s32B-b79K")
processor = CLIPProcessor.from_pretrained("laion/CLIP-ViT-H-14-laion2B-s32B-b79K")

# Define Pydantic Model for Structured API Response
class ImageAnalysisResponse(BaseModel):
    success: bool
    image_name: str
    predicted_damage: str
    recommended_repair: str
    name_of_product: str
    era: str
    material: str
    condition: str
    value: str

def compute_image_hash(image_path):
    """ Generates a stable hash based on the image content."""
    hasher = hashlib.sha256()
    with open(image_path, 'rb') as img_file:
        while chunk := img_file.read(8192):
            hasher.update(chunk)
    return hasher.hexdigest()

def stable_hash(hash_str, choices):
    """ Uses a consistent hash from image content to determine metadata values."""
    hash_val = int(hash_str, 16)
    return choices[hash_val % len(choices)]

def predict_material(image_path):
    """ Uses AI to predict the shoe material accurately."""
    image = Image.open(image_path).convert("RGB")
    inputs = processor(
        text=MATERIAL_LABELS,
        images=image,
        return_tensors="pt",
        padding=True
    )
    with torch.no_grad():
        outputs = model(
            pixel_values=inputs["pixel_values"],
            input_ids=inputs.get("input_ids"),
            attention_mask=inputs.get("attention_mask")
        )
    probs = outputs.logits_per_image.softmax(dim=1)  # Convert logits to probabilities
    predicted_idx = torch.argmax(probs, dim=1).item()
    return MATERIAL_LABELS[predicted_idx]

def process_image(image_path):
    """
    Uses AI to analyze the shoe image, predict damages and material, and return accurate repair recommendations.
    """
    try:
        # Compute a stable hash based on the image content
        image_hash = compute_image_hash(image_path)
        print(f"Processing image with hash: {image_hash[:10]}")  # Debug log

        # Open and preprocess the image
        image = Image.open(image_path).convert("RGB")
        inputs = processor(
            text=LABELS,
            images=image,
            return_tensors="pt",
            padding=True
        )

        # Run AI model for damage prediction
        with torch.no_grad():
            outputs = model(
                pixel_values=inputs["pixel_values"],
                input_ids=inputs.get("input_ids"),
                attention_mask=inputs.get("attention_mask")
            )
            
        probs = outputs.logits_per_image.softmax(dim=1)  # Convert logits to probabilities
        predicted_idx = torch.argmax(probs, dim=1).item()
        predicted_label = LABELS[predicted_idx]
        repair_suggestion = REPAIR_MAPPING.get(predicted_label, "Unknown Repair Suggestion")

        # AI-based shoe material classification
        predicted_material = predict_material(image_path)

        # Stable metadata generation based on image content hash
        return ImageAnalysisResponse(
            success=True,
            image_name=os.path.basename(image_path),
            predicted_damage=predicted_label,
            recommended_repair=repair_suggestion,
            name_of_product=f"Product-{int(image_hash[:8], 16) % 10000}",
            era=stable_hash(image_hash, ERA_MAPPING),
            material=predicted_material,  # AI-determined material
            condition=CONDITION_MAPPING.get(predicted_label, "Unknown"),
            value=stable_hash(image_hash, VALUE_MAPPING)
        )
    except Exception as e:
        print(f"Error: {str(e)}")
        raise RuntimeError(f"Error processing image: {str(e)}")

@app.post("/process-image/", response_model=ImageAnalysisResponse)
async def process_image_endpoint(file: UploadFile = File(...)):
    """
    Endpoint to upload and process an image, returning AI-generated shoe damage and material analysis.
    """
    try:
        # Save uploaded file temporarily
        file_path = os.path.join(UPLOAD_FOLDER, file.filename)
        with open(file_path, "wb") as buffer:
            buffer.write(await file.read())

        # Process image using AI model
        result = process_image(file_path)

        # Clean up
        os.remove(file_path)

        # **Force explicit JSON response with headers**
        return JSONResponse(
            content=result.dict(),
            headers={"Content-Type": "application/json"}
        )
    except Exception as e:
        return JSONResponse(content={"success": False, "error": str(e)}, status_code=500)


